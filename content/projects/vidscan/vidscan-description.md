<a href="#" onclick="document.querySelector('#menu-items li:nth-child(2) a').click(); return false;" class="back-button">&larr; Powrót do projektów</a>

# VidScan – 3D Model Generation from Video

## Project Overview

VidScan is designed to create 3D models from videos. The system processes video sequences by extracting frames, analyzing them with neural networks to detect objects and generate masks, and then using Meshroom software to create 3D models via photogrammetry.

This project was developed at vBionic primarily for medical applications. Its main objective was scanning limbs of amputees to produce precise 3D models of residual limbs (stumps) and healthy limbs (arms and legs). These models are utilized in prosthetic design and fitting.

## System Architecture

The system is modular, facilitating easy development and debugging of individual processing stages. It also allows updating specific modules (e.g., newer versions of Meshroom or neural networks). Incremental improvements can thus be efficiently implemented to enhance scanning quality.

The main system components include:
- Web server for managing user data and files
- Video processing module
- Neural network for object detection and mask generation
- 3D reconstruction module (Meshroom)

## Data Flow and Processing Stages

The VidScan pipeline comprises the following stages:

### 1. Frame Extraction

Input videos are processed by extracting frames at a specified frequency (default 5 FPS) and resizing them to a defined resolution (default 1280x720 px). Higher resolutions do not always improve quality but significantly increase processing time (proportional to the square of the resolution).

![Selected frames extracted from video](/content/projects/vidscan/media/collage_img.png)
*Selected frames extracted from the video recording*

Attention must be paid to filming techniques. Camera movements (smartphones) should be slow to avoid blurry images. Ideally, the camera or smartphone should support a 60 FPS mode. Blurred or noisy images significantly degrade model quality and prolong processing times, sometimes making it impossible to generate a 3D model.

### 2. Color Space Conversion

Extracted frames are converted from RGB to LAB color space.

![B component of LAB color space](/content/projects/vidscan/media/collage_lab_b.png)
*Example of the B component of LAB color space, which helps in distinguishing skin tones*

#### Importance of LAB Color Space

LAB color space was specifically chosen because:
- Brightness and color separation: LAB separates brightness (L channel) from color information (a and b channels), improving skin identification under varying lighting conditions.
- Consistency of skin tones: LAB color space maintains consistent skin tones despite brightness and ethnicity variations.
- Greater resistance to lighting variability: Separating brightness helps neural networks accurately segment skin by ignoring shadows and illumination variations.
- Improved boundary detection: LAB enhances differentiation of similarly colored areas, crucial for accurate segmentation of limbs and stumps.

### 3. Object Mask Generation

This module uses neural networks (U-Net/FlexNet) to detect objects and generate masks, identifying areas of interest and optionally cropping images.

![Masks generated by neural network](/content/projects/vidscan/media/collage_mask.png)
*Examples of masks generated by the neural network for object segmentation*

### 4. Background Removal (Optional)

This optional module removes backgrounds, leaving only objects identified by neural networks. Partial background removal using defined margins around masks is also supported.

### 5. 3D Model Generation

The module uses Meshroom software (based on AliceVision) to process frames and generate 3D models. This involves feature analysis, matching points across frames, Structure-from-Motion reconstruction, dense point cloud generation, and mesh creation. Processing is controlled via Python scripts, allowing parameter adjustments.

Processing time depends on the number and resolution of input images, and video quality (e.g., blurred images from fast camera movements). Processing times ranged from 20 minutes to 2 hours. Good image quality significantly reduces processing time.

Computations are performed on workstations with multi-threaded processors and RTX graphics cards.

### 6. Result Export

Generated 3D models are exported to OBJ/STL formats.

The OBJ format is advantageous as it is simple to generate, easy to analyze, and most importantly, allows texture attachment. In the application for which VidScan was created, texture is crucial as it enables verification of scan accuracy. It also allows checking the correctness of object dimensions.

OBJ also allows for simple conversion into data for PointNet networks, which can be used to clean the scan from environmental elements, so that the person receiving the scan doesn't have to do this manually.
The scan cleaning stage was a separate project; if you want to learn more details, check out the project "3D Scene Segmentation using Neural Networks."

![Skan 3D kikuta po amputacji](/content/projects/vidscan/media/scan_3D.png)
*Example of a resulting 3D scan of a residual limb after amputation*

## User Interface

A web-based interface allows:
- Management of patient/user data
- Video file uploads
- Processing status monitoring
- Downloading and viewing 3D models

## Technical Infrastructure

The project utilizes:
- Python for data processing and computer vision algorithms
- OpenCV and scikit-image for image processing
- PyTorch for neural network implementation
- Meshroom for photogrammetric 3D reconstruction
- Web server (Apache) with PHP, HTML, and CSS for the user interface

## Applications

VidScan is applicable in various fields, including:
- Medical documentation for amputees
- Personalized prosthetic design and fitting
- Monitoring stump shape changes over time
- Training medical staff
- Other applications in 3D modeling and digitization

## Personal Contributions
- Python Processing Modules (100%)
  - Video processing
  - Color conversion
  - Mask generation (network training, result processing)
  - Meshroom automation
  - Result conversion
- Module integration (100%)
- User interface (100%)
- FlexNet neural network training/adaptation (100%)
- FlexNet network architecture (20%) – The network was designed at vBionic, primarily by Jakub Slast. My contribution involved selecting network parameters, structural adjustments, and scripting for training and evaluation.

<style>
.back-button {
  display: inline-block;
  background-color: #333;
  color: #fff;
  padding: 8px 16px;
  border-radius: 4px;
  text-decoration: none;
  margin-bottom: 20px;
  transition: background-color 0.3s;
}

.back-button:hover {
  background-color: #555;
  text-decoration: none;
  color: #fff;
}

/* Style dla obrazków - ograniczone tylko do głównej treści */
.main-content img {
  max-width: 100%;
  height: auto;
  display: block;
  margin: 0 auto;
  border-radius: 4px;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
}

/* Styl dla podpisów pod obrazkami - ograniczone tylko do głównej treści */
.main-content img + em {
  display: block;
  text-align: center;
  font-size: 0.9em;
  color: #888;
  margin-top: 8px;
  margin-bottom: 20px;
}
</style>